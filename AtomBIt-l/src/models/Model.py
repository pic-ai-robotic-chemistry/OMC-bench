from typing import Dict, Optional, Tuple
from dataclasses import dataclass, field
import torch
import torch.nn as nn
import torch.nn.functional as F
from .Modules import (
    GeometricBasis, LeibnizCoupling, PhysicsGating, CartesianDensityBlock, LatentLongRange)
from src.utils import scatter_add, HTGPConfig

# ==========================================
# 7. ä¸»æ¨¡å‹ (Main Model)
# ==========================================
class HTGPModel(nn.Module):
    def __init__(self, config: HTGPConfig):
        super().__init__()
        self.cfg = config
        
        # ============================================================
        # ğŸ”¥ ä¿®æ”¹ 1: æ„å»ºåŸå­åºæ•°æ˜ å°„è¡¨ (Z-Mapper)
        # ============================================================
        # ä¼˜å…ˆä» config è·å–åŸå­åˆ—è¡¨ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤çš„å¸¸ç”¨æœ‰æœºå…ƒç´ åˆ—è¡¨
        # å¯¹åº”: H(1), B(5), C(6), N(7), O(8), F(9), P(15), S(16), Cl(17), Br(35), I(53)
        if hasattr(config, 'atom_types_map'):
            self.used_atomic_numbers = config.atom_types_map
        else:
            self.used_atomic_numbers = [1, 5, 6, 7, 8, 9, 15, 16, 17, 35, 53]
            
        num_actual_types = len(self.used_atomic_numbers) # é€šå¸¸ä¸º 11
        max_z = max(self.used_atomic_numbers)            # é€šå¸¸ä¸º 53

        # æ³¨å†Œæ˜ å°„è¡¨ buffer (ä¼šè‡ªåŠ¨è½¬åˆ° GPUï¼Œä½†ä¸æ›´æ–°æ¢¯åº¦)
        # åˆå§‹åŒ–ä¸º -1ï¼Œæ–¹ä¾¿åç»­æ£€æŸ¥éæ³•åŸå­
        self.register_buffer('z_mapper', torch.full((max_z + 1,), -1, dtype=torch.long))
        
        # å¡«å……æ˜ å°„: z -> idx (ä¾‹å¦‚ 53 -> 10)
        for idx, z in enumerate(self.used_atomic_numbers):
            self.z_mapper[z] = idx

        # ============================================================
        # ğŸ”¥ ä¿®æ”¹ 2: Embedding å°ºå¯¸ç¼©å°
        # ============================================================
        # Embedding: åªåˆ†é… 11 è¡Œå‚æ•°ï¼Œè€Œä¸æ˜¯ 60 è¡Œ
        self.embedding = nn.Embedding(num_actual_types, config.hidden_dim)
        nn.init.normal_(self.embedding.weight, std=0.1)
        
        # Components (ä¿æŒä¸å˜)
        self.geom_basis = GeometricBasis(config)
        
        self.layers = nn.ModuleList()
        for _ in range(config.num_layers):
            self.layers.append(nn.ModuleDict({
                'coupling': LeibnizCoupling(config),
                'gating': PhysicsGating(config),
                'density': CartesianDensityBlock(config),
                'readout': nn.Sequential(
                    nn.Linear(config.hidden_dim, config.hidden_dim),
                    nn.SiLU(),
                    nn.Linear(config.hidden_dim, 1)
                )
            }))
            
        if config.use_long_range:
            self.long_range = LatentLongRange(config)
            
        # Atomic Ref: åŒæ ·ç¼©å°å°ºå¯¸
        self.atomic_ref = nn.Embedding(num_actual_types, 1)
        nn.init.zeros_(self.atomic_ref.weight)
            
    def forward(self, data, capture_weights=False, capture_descriptors=False):
        if capture_descriptors:
            self.all_layer_descriptors = []
        
        # ============================================================
        # ğŸ”¥ ä¿®æ”¹ 3: Forward ä¸­åº”ç”¨æ˜ å°„
        # ============================================================
        # è·å–åŸå§‹åŸå­åºæ•° (N,)
        z_raw = data.z
        
        # è½¬æ¢ä¸ºç¨ å¯†ç´¢å¼• (N,) -> [0, 2, 10, ...]
        z_idx = self.z_mapper[z_raw]
        
        # (å¯é€‰) å®‰å…¨æ£€æŸ¥: å¦‚æœæ•°æ®é‡Œæ··å…¥äº†æœªå®šä¹‰çš„åŸå­ (å¦‚ Fe=26)ï¼Œè¿™é‡Œä¼šæ˜¯ -1
        # if (z_idx == -1).any():
        #    raise ValueError(f"Input contains undefined atomic numbers! Supported: {self.used_atomic_numbers}")

        # 1. å‡ ä½•è®¡ç®—
        row, col = data.edge_index
        # å¤„ç† shifts_int (PBC)
        if hasattr(data, 'shifts_int') and data.shifts_int is not None:
            batch_cell = data.cell[data.batch[row]]          # (E, 3, 3)
            current_shifts = torch.bmm(
                data.shifts_int.unsqueeze(1), batch_cell
            ).squeeze(1)                                     # (E, 3)
        else:
            current_shifts = torch.zeros(
                (row.size(0), 3),
                device=data.pos.device,
                dtype=data.pos.dtype
            )

        vec_ij = data.pos[col] - data.pos[row] + current_shifts
        d_ij = torch.norm(vec_ij, dim=-1).clamp(min=1e-8)

        basis_edges, r_hat = self.geom_basis(vec_ij, d_ij)

        # 2. çŠ¶æ€åˆå§‹åŒ– (ä½¿ç”¨ z_idx)
        h0 = self.embedding(z_idx) # (N, F) -> ä½¿ç”¨æ˜ å°„åçš„ç´¢å¼•
        h1 = None 
        h2 = None
        
        total_energy = 0.0
        
        # 3. å±‚çº§ä¼ é€’
        for layer in self.layers:
            # A. è±å¸ƒå°¼èŒ¨æ¶ˆæ¯ç”Ÿæˆ
            node_feats = {0: h0, 1: h1, 2: h2}
            raw_msgs = layer['coupling'](node_feats, basis_edges, data.edge_index)
            
            # B. ç‰©ç†é—¨æ§
            gated_msgs = layer['gating'](raw_msgs, h0, basis_edges[0], r_hat, h1, data.edge_index, capture_weights=capture_weights)
            
            # C. å¯†åº¦èšåˆä¸æ›´æ–°
            # æ³¨æ„: è¿™é‡Œ data.z.size(0) æ˜¯èŠ‚ç‚¹æ€»æ•°ï¼Œä¿æŒä¸å˜
            delta_h0, delta_h1, delta_h2 = layer['density'](gated_msgs, row, data.z.size(0))

            # D. æ®‹å·®æ›´æ–° (Residual Update)
            h0 = h0 + delta_h0

            if self.cfg.use_L1:
                if h1 is None:
                    h1 = delta_h1 # ç¬¬ä¸€å±‚ç›´æ¥èµ‹å€¼
                elif delta_h1 is not None:
                    h1 = h1 + delta_h1 # åç»­å±‚ç´¯åŠ 

            if self.cfg.use_L2:
                if h2 is None:
                    h2 = delta_h2
                elif delta_h2 is not None:
                    h2 = h2 + delta_h2

            # h0 h1 h2ä¿å­˜
            if capture_descriptors:
                current_layer_feats = {
                    'h0': h0.detach().cpu(), # âš ï¸ å¿…é¡» detach å¹¶è½¬åˆ° cpuï¼Œå¦åˆ™æ˜¾å­˜çˆ†ç‚¸
                }
                if self.cfg.use_L1 and h1 is not None:
                    current_layer_feats['h1'] = h1.detach().cpu()
                if self.cfg.use_L2 and h2 is not None:
                    current_layer_feats['h2'] = h2.detach().cpu()
                
                self.all_layer_descriptors.append(current_layer_feats)

            # E. èƒ½é‡è¯»å‡º
            atomic_energy = layer['readout'](h0)
            total_energy = total_energy + scatter_add(atomic_energy, data.batch, dim=0, dim_size=data.num_graphs)
            
        # 4. é•¿ç¨‹ä¿®æ­£
        if self.cfg.use_long_range and self.cfg.use_L1 and h1 is not None:
            e_long = self.long_range(h1, data.pos, data.batch)
            total_energy = total_energy + e_long
            
        # Atomic Ref (ä½¿ç”¨ z_idx)
        total_energy = total_energy + scatter_add(self.atomic_ref(z_idx), data.batch, dim=0, dim_size=data.num_graphs)

        return total_energy

    # ============================================================
    # ğŸ”¥ æ–°å¢: å¤–éƒ¨ E0 åŠ è½½è¾…åŠ©å‡½æ•° (ä¾› train.py è°ƒç”¨)
    # ============================================================
    def load_external_e0(self, e0_dict, device=None, verbose=True, rank = 0):
        """
        ä»å­—å…¸åŠ è½½ E0ï¼Œè‡ªåŠ¨å¤„ç†åŸå­åºæ•°åˆ°å†…éƒ¨ç´¢å¼•çš„æ˜ å°„ã€‚
        """
        if device is None:
            device = self.atomic_ref.weight.device
            
        count = 0
        with torch.no_grad():
            # å°† mapper è½¬åˆ° CPU ä»¥ä¾¿å¿«é€ŸæŸ¥è¡¨ (Python int loop)
            mapper_cpu = self.z_mapper.cpu()
            
            for z, e in e0_dict.items():
                z_raw = int(z)
                # æ£€æŸ¥ z æ˜¯å¦åœ¨ mapper èŒƒå›´å†…
                if z_raw < len(mapper_cpu):
                    mapped_idx = mapper_cpu[z_raw].item()
                    # å¦‚æœæ˜ å°„æœ‰æ•ˆ (!= -1)
                    if mapped_idx != -1:
                        val = torch.tensor(e, dtype=torch.float32, device=device)
                        self.atomic_ref.weight[mapped_idx] = val
                        count += 1
                        
        # å†»ç»“å‚æ•°ï¼Œä¸å‚ä¸è®­ç»ƒ
        self.atomic_ref.weight.requires_grad = False
        if verbose and rank == 0:
            print(f"ğŸ”’ [Model Internal] Injected E0 for {count} elements.")