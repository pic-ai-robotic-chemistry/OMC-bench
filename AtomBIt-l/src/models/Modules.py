import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Dict, Optional, Tuple, List
from src.utils import scatter_add, scatter_mean, HTGPConfig

# ==========================================
# ğŸ”¥ æ ¸å¿ƒ JIT æ•°å­¦å¼•æ“ (å®‰å…¨åŠ é€ŸåŒº)
# ==========================================

@torch.jit.script
def compute_bessel_math(d: torch.Tensor, r_max: float, freq: torch.Tensor) -> torch.Tensor:
    d_scaled = d / r_max
    prefactor = (2.0 / r_max) ** 0.5
    return prefactor * torch.sin(freq * d_scaled) / (d + 1e-6)
 
@torch.jit.script
def compute_envelope_math(d: torch.Tensor, r_cut: float) -> torch.Tensor:
    x = d / r_cut
    x = torch.clamp(x, min=0.0, max=1.0)
    return 1.0 - 10.0 * x**3 + 15.0 * x**4 - 6.0 * x**5

@torch.jit.script
def compute_l2_basis(rbf_feat: torch.Tensor, r_hat: torch.Tensor) -> torch.Tensor:
    outer = r_hat.unsqueeze(2) * r_hat.unsqueeze(1) 
    eye = torch.eye(3, dtype=r_hat.dtype, device=r_hat.device).unsqueeze(0)
    trace_less = outer - (1.0/3.0) * eye
    return rbf_feat.unsqueeze(1).unsqueeze(1) * trace_less.unsqueeze(-1)

@torch.jit.script
def compute_invariants(den0: Optional[torch.Tensor], 
                       den1: Optional[torch.Tensor], 
                       den2: Optional[torch.Tensor]) -> torch.Tensor:
    # âœ… ä¿®å¤ï¼šä½¿ç”¨æ ‡å‡†ç±»å‹æ ‡æ³¨
    invariants: List[torch.Tensor] = []
    
    if den0 is not None:
        invariants.append(den0)
        
    if den1 is not None:
        sq_sum = torch.sum(den1.pow(2), dim=1) 
        norm = torch.sqrt(sq_sum + 1e-8)
        invariants.append(norm)
        
    if den2 is not None:
        sq_sum = torch.sum(den2.pow(2), dim=(1, 2))
        norm = torch.sqrt(sq_sum + 1e-8)
        invariants.append(norm)
        
    if len(invariants) > 0:
        return torch.cat(invariants, dim=-1)
    else:
        # è¿”å›ç©º Tensor (æ³¨æ„å¤„ç† device é—®é¢˜ï¼Œæœ€å¥½ç”±å¤–éƒ¨ä¿è¯ invariants ä¸ä¸ºç©º)
        return torch.zeros(0) 

@torch.jit.script
def compute_gating_projections(h_node1: torch.Tensor, 
                               r_hat: torch.Tensor, 
                               scalar_basis: torch.Tensor,
                               src: torch.Tensor, 
                               dst: torch.Tensor) -> torch.Tensor:
    r_hat_uns = r_hat.unsqueeze(-1)
    p_src = torch.sum(h_node1[src] * r_hat_uns, dim=1)
    p_dst = torch.sum(h_node1[dst] * r_hat_uns, dim=1)
    return torch.cat([scalar_basis, p_src, p_dst], dim=-1)


# ==========================================
# ğŸ§© æ¨¡å—å®šä¹‰ (æ™®é€š nn.Module åŒº)
# ==========================================

class BesselBasis(nn.Module): 
    def __init__(self, r_max: float, num_basis: int = 8):
        super().__init__()
        self.r_max = float(r_max)
        self.num_basis = int(num_basis)
        self.register_buffer("freq", torch.arange(1, num_basis + 1).float() * np.pi)

    def forward(self, d: torch.Tensor) -> torch.Tensor:
        return compute_bessel_math(d, self.r_max, self.freq)

class PolynomialEnvelope(nn.Module):
    def __init__(self, r_cut: float, p: int = 5):
        super().__init__()
        self.r_cutoff = float(r_cut)
        self.p = int(p)
    
    def forward(self, d_ij: torch.Tensor) -> torch.Tensor:
        return compute_envelope_math(d_ij, self.r_cutoff)

class GeometricBasis(nn.Module):
    def __init__(self, config: HTGPConfig):
        super().__init__()
        self.cfg = config
        self.rbf = BesselBasis(config.cutoff, config.num_rbf)
        self.envelope = PolynomialEnvelope(r_cut=config.cutoff)
        self.rbf_mlp = nn.Sequential(
            nn.Linear(config.num_rbf, config.hidden_dim),
            nn.SiLU(),
            nn.Linear(config.hidden_dim, config.hidden_dim)
        )

    def forward(self, vec_ij, d_ij):
        raw_rbf = self.rbf_mlp(self.rbf(d_ij.unsqueeze(-1)))
        env = self.envelope(d_ij)
        rbf_feat = raw_rbf * env.unsqueeze(-1)

        # âš ï¸ r_hat è®¡ç®—å¿…é¡»åœ¨ Python å±‚ä¿ç•™ï¼Œç¡®ä¿æ¢¯åº¦ä¼ å¯¼
        r_hat = vec_ij / (d_ij.unsqueeze(-1) + 1e-6)
        
        basis = {}
        basis[0] = rbf_feat
        
        if self.cfg.use_L1 or self.cfg.use_L2:
            basis[1] = rbf_feat.unsqueeze(1) * r_hat.unsqueeze(-1)
            
        if self.cfg.use_L2:
            basis[2] = compute_l2_basis(rbf_feat, r_hat)
            
        return basis, r_hat

class LeibnizCoupling(nn.Module): 
    def __init__(self, config: HTGPConfig):
        super().__init__()
        self.cfg = config
        self.F = config.hidden_dim
        self.path_weights = nn.ModuleDict()
        
        for path_key, active in config.active_paths.items():
            if not active: continue
            l_in, l_edge, l_out, _ = path_key
            if (l_in == 2 or l_edge == 2 or l_out == 2) and not config.use_L2: continue
            if (l_in == 1 or l_edge == 1 or l_out == 1) and not config.use_L1: continue
                
            name = f"{l_in}_{l_edge}_{l_out}_{path_key[3]}"
            self.path_weights[name] = nn.Linear(self.F, self.F, bias=False)

        self.inv_sqrt_f = self.F ** -0.5

    def forward(self, h_nodes: Dict[int, torch.Tensor], basis_edges: Dict[int, torch.Tensor], edge_index):
        src, _ = edge_index
        messages: Dict[int, List[torch.Tensor]] = {0: [], 1: [], 2: []}
        
        for path_key, active in self.cfg.active_paths.items():
            if not active: continue
            l_in, l_edge, l_out, op_type = path_key
            
            if basis_edges.get(l_edge) is None: continue
            
            layer_name = f"{l_in}_{l_edge}_{l_out}_{op_type}"
            if layer_name not in self.path_weights: continue
            
            if h_nodes.get(l_in) is None: continue 
            else: inp = h_nodes[l_in]
            
            h_src = inp[src]
            h_trans = self.path_weights[layer_name](h_src)
            geom = basis_edges[l_edge]
            res = None
            
            # --- Operation Logic ---
            if op_type == 'prod':
                if l_in == 0 and l_edge == 0: res = h_trans * geom
                elif l_in == 0 and l_edge == 1: res = h_trans.unsqueeze(1) * geom
                elif l_in == 0 and l_edge == 2: res = h_trans.unsqueeze(1).unsqueeze(1) * geom
                elif l_in == 1 and l_edge == 0: res = h_trans * geom.unsqueeze(1)
                elif l_in == 2 and l_edge == 0: res = h_trans * geom.unsqueeze(1).unsqueeze(1)
            elif op_type == 'dot':
                res = torch.sum(h_trans * geom, dim=1)
            elif op_type == 'cross':
                g = geom
                if g.dim() == 2: g = g.unsqueeze(-1)
                res = torch.linalg.cross(h_trans, g, dim=1)
            elif op_type == 'outer':
                outer = h_trans.unsqueeze(2) * geom.unsqueeze(1)
                trace = torch.einsum('eiif->ef', outer)
                eye = torch.eye(3, device=outer.device).view(1, 3, 3, 1)
                res = outer - (1.0/3.0) * trace.unsqueeze(1).unsqueeze(1) * eye
            elif op_type == 'mat_vec':
                res = torch.einsum('eijf, ejf -> eif', h_trans, geom)
            elif op_type == 'vec_mat':
                res = torch.einsum('eif, eijf -> ejf', h_trans, geom)
            elif op_type == 'double_dot':
                res = torch.sum(h_trans * geom, dim=(1, 2))
            elif op_type == 'mat_mul_sym':
                raw = torch.einsum('eikf, ekjf -> eijf', h_trans, geom)
                sym = 0.5 * (raw + raw.transpose(1, 2))
                trace = torch.einsum('eiif->ef', sym)
                eye = torch.eye(3, device=sym.device).view(1, 3, 3, 1)
                res = sym - (1.0/3.0) * trace.unsqueeze(1).unsqueeze(1) * eye

            if res is not None:
                messages[l_out].append(res * self.inv_sqrt_f)
                
        final_msgs: Dict[int, Optional[torch.Tensor]] = {}
        for l in [0, 1, 2]:
            final_msgs[l] = sum(messages[l]) if len(messages[l]) > 0 else None
        return final_msgs

class PhysicsGating(nn.Module):
    def __init__(self, config: HTGPConfig):
        super().__init__()
        self.cfg = config
        self.F = config.hidden_dim
        
        self.W_query = nn.Linear(self.F, self.F, bias=False)
        self.W_key = nn.Linear(self.F, self.F, bias=False)
        
        self.phys_bias_mlp = nn.Sequential(
            nn.Linear(3 * self.F, self.F), 
            nn.SiLU(),            
            nn.Linear(self.F, 3 * self.F) 
        )
        self.channel_mixer = nn.Linear(self.F, 3 * self.F, bias=False)
        self.gate_scale = nn.Parameter(torch.ones(1) * 2.0)

    def forward(self, msgs, h_node0, scalar_basis, r_hat, h_node1, edge_index, capture_weights=False):
        if not self.cfg.use_gating: return msgs
        
        src, dst = edge_index
        
        if h_node1 is not None:
            phys_input = compute_gating_projections(h_node1, r_hat, scalar_basis, src, dst)
            split_idx = scalar_basis.shape[-1]
            p_ij = phys_input[:, split_idx:]        
        else:
            p_ij = torch.zeros((scalar_basis.shape[0], 2 * self.F), device=scalar_basis.device)
            phys_input = torch.cat([scalar_basis, p_ij], dim=-1)

        q = self.W_query(h_node0[dst]) 
        k = self.W_key(h_node0[src])   
        chem_score = q * k             
        chem_logits = self.channel_mixer(chem_score)
        phys_logits = self.phys_bias_mlp(phys_input)
        
        raw_gates = chem_logits + phys_logits
        gates = torch.sigmoid(raw_gates) * self.gate_scale
        
        if capture_weights: self.scalar_basis_captured = scalar_basis.detach()
        if capture_weights: self.p_ij_captured = p_ij.detach()
        if capture_weights: self.chem_logits_captured = chem_logits.detach()
        if capture_weights: self.phys_logits_captured = phys_logits.detach()

        g_list = torch.split(gates, self.F, dim=-1)
        g0, g1, g2 = [g.contiguous() for g in g_list]

        if capture_weights: self.g0_captured = g0.detach()
        if capture_weights: self.g1_captured = g1.detach()
        if capture_weights: self.g2_captured = g2.detach()
        
        out_msgs: Dict[int, torch.Tensor] = {}
        if 0 in msgs and msgs[0] is not None: out_msgs[0] = msgs[0] * g0
        if 1 in msgs and msgs[1] is not None: out_msgs[1] = msgs[1] * g1.unsqueeze(1)
        if 2 in msgs and msgs[2] is not None: out_msgs[2] = msgs[2] * g2.unsqueeze(1).unsqueeze(1)
            
        return out_msgs

class CartesianDensityBlock(nn.Module):
    def __init__(self, config: HTGPConfig):
        super().__init__()
        self.F = config.hidden_dim
        self.cfg = config
        
        in_dim = 0
        if config.use_L0: in_dim += self.F
        if config.use_L1: in_dim += self.F
        if config.use_L2: in_dim += self.F 
        
        self.scalar_update_mlp = nn.Sequential(
            nn.Linear(in_dim, self.F),
            nn.SiLU(),
            nn.Linear(self.F, self.F)
        )

        if config.use_L1: self.L1_linear = nn.Linear(self.F, self.F, bias=False)
        if config.use_L2: self.L2_linear = nn.Linear(self.F, self.F, bias=False)

        scale_out_dim = 0
        if config.use_L1: scale_out_dim += self.F
        if config.use_L2: scale_out_dim += self.F
        
        if scale_out_dim > 0:
            self.scale_mlp = nn.Sequential(
                nn.Linear(self.F, self.F),
                nn.SiLU(),
                nn.Linear(self.F, scale_out_dim)
            )
        else:
            self.scale_mlp = None
 
        self.inv_sqrt_deg = 1.0 / (config.avg_neighborhood ** 0.5)

    def forward(self, msgs: Dict[int, torch.Tensor], index: torch.Tensor, num_nodes: int):
        # 1. å¯†åº¦èšåˆ
        # âœ… ä¿®æ­£ï¼šæ ‡å‡†ç±»å‹æ ‡æ³¨ï¼Œæ˜ç¡® None
        densities: Dict[int, Optional[torch.Tensor]] = {}
        densities[0], densities[1], densities[2] = None, None, None

        for l in [0, 1, 2]:
            if l in msgs and msgs[l] is not None:
                agg = scatter_add(msgs[l], index, dim=0, dim_size=num_nodes)
                densities[l] = agg * self.inv_sqrt_deg 
            else:
                densities[l] = None

        # 2. æå–ä¸å˜é‡
        concat = compute_invariants(densities[0], densities[1], densities[2])

        # 3. æ ‡é‡æ›´æ–°
        # âœ… ä¿®æ­£ï¼šä½¿ç”¨ index.device é¿å…æ­§ä¹‰æŠ¥é”™
        if concat.numel() > 0:
            delta_h0 = self.scalar_update_mlp(concat)
        else:
            delta_h0 = torch.zeros((num_nodes, self.F), device=index.device)

        # 4. çŸ¢é‡æ›´æ–°
        delta_h1 = None
        delta_h2 = None

        if self.scale_mlp is not None:
            scales = self.scale_mlp(delta_h0)
            curr_dim = 0
            
            if self.cfg.use_L1 and densities[1] is not None:
                alpha1 = scales[:, curr_dim : curr_dim + self.F] 
                h1_mixed = self.L1_linear(densities[1])
                delta_h1 = h1_mixed * alpha1.unsqueeze(1)
                curr_dim += self.F
                
            if self.cfg.use_L2 and densities[2] is not None:
                alpha2 = scales[:, curr_dim : curr_dim + self.F]
                h2_mixed = self.L2_linear(densities[2])
                delta_h2 = h2_mixed * alpha2.unsqueeze(1).unsqueeze(1)

        return delta_h0, delta_h1, delta_h2

# ==========================================
# 6. é•¿ç¨‹åœº (Latent Long Range) - Ablation Ready
# ==========================================
class LatentLongRange(nn.Module):
    def __init__(self, config: HTGPConfig):
        super().__init__()
        self.cfg = config
        self.F = config.hidden_dim
        
        # ç‰©ç†å¸¸æ•°: Coulomb constant in eV * A
        self.KE = 14.3996 
        
        # --- 1. ç”µè·é¢„æµ‹ç½‘ç»œ (h0 -> q) ---
        if config.use_charge:
            self.q_proj = nn.Sequential(
                nn.Linear(self.F, self.F),
                nn.SiLU(),
                nn.Linear(self.F, 1, bias=False) # æ— åç½®ï¼Œç¡®ä¿ç©ºç‰¹å¾è¾“å‡º0ç”µè·
            )
            # å¯å­¦ä¹ çš„é«˜æ–¯åˆ†å¸ƒå®½åº¦ sigmaï¼Œåˆå§‹å€¼è®¾ä¸º 1.0 Ã…
            # è¿™å†³å®šäº†é•¿ç¨‹å’ŒçŸ­ç¨‹çš„"äº¤æ¥ç‚¹"
            self.sigma = nn.Parameter(torch.tensor(1.0))

        # --- 2. èŒƒå¾·åå‚æ•°é¢„æµ‹ (h0 -> C6, Rvdw) ---
        if config.use_vdw:
            self.vdw_proj = nn.Sequential(
                nn.Linear(self.F, self.F),
                nn.SiLU(),
                nn.Linear(self.F, 2) # è¾“å‡º [C6ç³»æ•°, èŒƒå¾·ååŠå¾„]
            )

        # --- 3. å¶æçŸ©é¢„æµ‹ (h1 -> mu) ---
        if config.use_dipole:
            self.mu_proj = nn.Linear(self.F, 1, bias=False)

    def forward(self, h0, h1, pos, batch):
        """
        è¾“å…¥:
            h0: (N, F) æ ‡é‡ç‰¹å¾
            h1: (N, 3, F) çŸ¢é‡ç‰¹å¾
            pos: (N, 3) åŸå­åæ ‡
            batch: (N,) æ‰¹æ¬¡ç´¢å¼•
        """
        energy_total = 0.0
        
        # ---------------------------------------------------------
        # æ„å»ºå…¨è¿æ¥å‡ ä½•å›¾ (O(N^2))
        # ---------------------------------------------------------
        # 1. è®¡ç®—æ‰€æœ‰åŸå­å¯¹çš„åæ ‡å·® (N, N, 3)
        diff = pos.unsqueeze(1) - pos.unsqueeze(0) 
        
        # 2. è®¡ç®—è·ç¦»å¹³æ–¹ (N, N)
        dist_sq = torch.sum(diff**2, dim=-1)
        
        # 3. è®¡ç®—è·ç¦» (N, N)ï¼ŒåŠ  epsilon é˜²æ­¢é™¤é›¶æ¢¯åº¦çˆ†ç‚¸
        dist = torch.sqrt(dist_sq + 1e-8)
        
        # 4. æ„å»º Mask: 
        # batch_mask: åªæœ‰åŒ batch çš„åŸå­æ‰è®¡ç®—
        # diag_mask: æ’é™¤è‡ªå·±å’Œè‡ªå·±è®¡ç®— (å¯¹è§’çº¿)
        batch_mask = (batch.unsqueeze(1) == batch.unsqueeze(0))
        diag_mask = torch.eye(pos.size(0), device=pos.device, dtype=torch.bool)
        valid_mask = batch_mask & (~diag_mask)

        # é¢„è®¡ç®—å€’æ•°ï¼Œå‡å°‘é™¤æ³•æ¬¡æ•°
        inv_dist = 1.0 / dist
        
        # ---------------------------------------------------------
        # æ¨¡å— 1: é™ç”µåŠ› (Electrostatics with erf Screening)
        # å…¬å¼: E = k * q_i * q_j / r * erf(r / (sqrt(2)*sigma))
        # ---------------------------------------------------------
        if self.cfg.use_charge:
            # é¢„æµ‹ç”µè·
            q = self.q_proj(h0) # (N, 1)
            
            # [ç‰©ç†çº¦æŸ] å¼ºåˆ¶ç”µè·ä¸­æ€§: æ¯ä¸ªåˆ†å­çš„æ€»ç”µè·å½’é›¶
            batch_q_mean = scatter_mean(q, batch, dim=0)
            q = q - batch_q_mean[batch]

            # ç”µè·ä¹˜ç§¯ q_i * q_j (N, N)
            qq = q @ q.t()
            
            # è®¡ç®—å±è”½å› å­ erf
            # è¿™é‡Œçš„ math.sqrt(2) æºè‡ªé«˜æ–¯ç§¯åˆ†çš„æ ‡å‡†å½¢å¼
            scaled_r = dist / (math.sqrt(2) * self.sigma)
            shielding = torch.erf(scaled_r)
            
            # ç»„åˆå…¬å¼
            # valid_mask ç¡®ä¿ä¸è®¡ç®—ä¸åŒåˆ†å­é—´å’Œè‡ªç›¸äº’ä½œç”¨
            E_coul = torch.sum(qq * inv_dist * shielding * valid_mask)
            
            # ä¹˜ä»¥ 0.5 (é¿å… i-j å’Œ j-i é‡å¤è®¡ç®—) å’Œ åº“ä»‘å¸¸æ•°
            energy_total += 0.5 * self.KE * E_coul

        # ---------------------------------------------------------
        # æ¨¡å— 2: èŒƒå¾·ååŠ› (VdW with Becke-Johnson Damping)
        # å…¬å¼: E = - C6 / (r^6 + f(R_vdw)^6)
        # ---------------------------------------------------------
        if self.cfg.use_vdw:
            # é¢„æµ‹å‚æ•°ï¼Œä½¿ç”¨ Softplus ç¡®ä¿ä¸ºæ­£æ•°
            vdw_params = self.vdw_proj(h0)
            c6 = F.softplus(vdw_params[:, 0:1])      # (N, 1)
            r_vdw = F.softplus(vdw_params[:, 1:2])   # (N, 1)
            
            # ç»„åˆè§„åˆ™ (Combination Rules)
            # C6_ij = sqrt(C6_i * C6_j)
            c6_ij = torch.sqrt(c6 @ c6.t())
            # R_vdw_ij = sqrt(R_i * R_j)
            r_vdw_ij = torch.sqrt(r_vdw @ r_vdw.t())
            
            # è®¡ç®— r^6
            dist6 = dist_sq ** 3
            
            # æ„é€  BJ é˜»å°¼åˆ†æ¯
            # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šå½“ r å¾ˆå°æ—¶ï¼Œåˆ†æ¯è¶‹å‘äº r_vdw^6 (å¸¸æ•°)ï¼Œé¿å…æ— ç©·å¤§
            # å½“ r å¾ˆå¤§æ—¶ï¼Œåˆ†æ¯è¶‹å‘äº r^6ï¼Œæ¢å¤æ ‡å‡†èŒƒå¾·åè¡°å‡
            damping = dist6 + (r_vdw_ij ** 6)
            
            # è®¡ç®—èƒ½é‡ (æ³¨æ„ç¬¦å·æ˜¯è´Ÿçš„ï¼Œå¸å¼•åŠ›)
            E_vdw = -torch.sum((c6_ij / damping) * valid_mask)
            
            energy_total += 0.5 * E_vdw

        # ---------------------------------------------------------
        # æ¨¡å— 3: å¶æçŸ©ç›¸äº’ä½œç”¨ (Dipole-Dipole)
        # ---------------------------------------------------------
        if self.cfg.use_dipole and h1 is not None:
            # h1 å½¢çŠ¶ (N, 3, F) -> æŠ•å½± -> (N, 3)
            mu = self.mu_proj(h1).squeeze(-1)
            
            # è®¡ç®— mu_i . mu_j
            mu_dot_mu = mu @ mu.t() # (N, N)
            
            # è®¡ç®—æ–¹å‘å‘é‡ n_ij = r_ij / r
            n_ij = diff * inv_dist.unsqueeze(-1) # (N, N, 3)
            
            # è®¡ç®— (mu_i . n_ij)
            # (N, 1, 3) * (N, N, 3) -> sum -> (N, N)
            mu_dot_n_i = torch.sum(mu.unsqueeze(1) * n_ij, dim=-1)
            
            # è®¡ç®— (mu_j . n_ij)
            # æ³¨æ„: n_ji = -n_ij, æ‰€ä»¥ mu_j . n_ij = - (mu_j . n_ji)
            # åˆ©ç”¨çŸ©é˜µè½¬ç½®æ€§è´¨: A_ij = mu_i . n_ij, é‚£ä¹ˆ A_ji = mu_j . n_ji
            # æ‰€ä»¥ mu_dot_n_j = - mu_dot_n_i.t()
            mu_dot_n_j = -mu_dot_n_i.t()
            
            # ç»„åˆé¡¹: (mu_i.mu_j) - 3(mu_i.n)(mu_j.n)
            angular_term = mu_dot_mu - 3 * mu_dot_n_i * mu_dot_n_j
            
            # å¾„å‘é¡¹: 1 / r^3
            # åŒæ ·éœ€è¦ erf å±è”½é˜²æ­¢çŸ­ç¨‹å‘æ•£ (LES ç†è®ºåŒæ ·é€‚ç”¨å¶æ)
            # ä½¿ç”¨ erf(x)^3 æ˜¯ä¸€ç§å¸¸è§çš„å¶æå±è”½è¿‘ä¼¼
            r_scaled = dist / self.sigma
            shielding_dip = torch.erf(r_scaled) ** 3
            radial_term = (inv_dist ** 3) * shielding_dip
            
            E_dip = torch.sum(angular_term * radial_term * valid_mask)
            energy_total += 0.5 * self.KE * E_dip

        # è¿”å›æ€»èƒ½é‡ï¼Œä¹˜ä»¥æ­¤å¤„çš„ç¼©æ”¾ç³»æ•°å¯ä»¥è®©è®­ç»ƒåˆæœŸæ›´ç¨³å®š
        return energy_total * self.cfg.long_range_scale