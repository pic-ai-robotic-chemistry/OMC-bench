import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import os
import csv
from tqdm.auto import tqdm
from src.utils import scatter_add
import torch.distributed as dist
from torch_ema import ExponentialMovingAverage

# ğŸ”¥ ä¼˜åŒ– 1: JIT ç¼–è¯‘ Loss å‡½æ•° (ç®—å­èåˆï¼ŒåŠ é€Ÿè®¡ç®—)
@torch.jit.script
def conditional_huber_loss(pred: torch.Tensor, target: torch.Tensor, base_delta: float = 0.01) -> torch.Tensor:
    """
    è‡ªé€‚åº” Huber Loss (JIT Optimized)
    """
    # è®¡ç®—æ¯ä¸ªåŸå­çš„å—åŠ›æ¨¡é•¿ (N, 1)
    force_norm = torch.norm(target, dim=1, keepdim=True)
    
    # åˆå§‹åŒ–ç¼©æ”¾å› å­
    delta_scale = torch.ones_like(force_norm)
    
    # é˜¶æ¢¯å¼é™çº§ç­–ç•¥
    mask_100_200 = (force_norm >= 100) & (force_norm < 200)
    delta_scale[mask_100_200] = 0.7
    
    mask_200_300 = (force_norm >= 200) & (force_norm < 300)
    delta_scale[mask_200_300] = 0.4
    
    mask_300 = (force_norm >= 300)
    delta_scale[mask_300] = 0.1
    
    # è®¡ç®—æœ€ç»ˆçš„ delta
    adaptive_delta = base_delta * delta_scale
    
    # æ‰‹åŠ¨å®ç° Huber è®¡ç®—é€»è¾‘
    error = pred - target
    abs_error = torch.abs(error)
    
    # åˆ¤å®š MSE åŒºåŸŸ
    is_mse = abs_error < adaptive_delta
    
    loss_mse = 0.5 * error ** 2
    loss_l1 = adaptive_delta * (abs_error - 0.5 * adaptive_delta)
    
    # ç»„åˆå¹¶å–å¹³å‡
    loss = torch.where(is_mse, loss_mse, loss_l1)
    return loss.mean()

class PotentialTrainer:
    def __init__(self, model, total_steps, max_lr=1e-3, device='cuda', checkpoint_dir='checkpoints', epochs=15, **kwargs):
        """
        Args:
            total_steps: æ€»è®­ç»ƒæ­¥æ•°
            epochs: æ€»è®­ç»ƒè½®æ¬¡
        """
        self.device = device
        self.model = model.to(self.device)
        
        # 1. ä¼˜åŒ–å™¨é…ç½®
        self.optimizer = optim.AdamW(
            model.parameters(), 
            lr=max_lr, # åˆå§‹å­¦ä¹ ç‡
            weight_decay=1e-4, # L2 æ­£åˆ™åŒ–
            betas=(0.9, 0.999), # é»˜è®¤å€¼
            amsgrad=True # ä½¿ç”¨ AMSGrad å˜ä½“
        )

        # 2. EMA (æŒ‡æ•°ç§»åŠ¨å¹³å‡)
        self.ema = ExponentialMovingAverage(self.model.parameters(), decay=0.999)

        # 3. å­¦ä¹ ç‡è°ƒåº¦å™¨
        last_step = kwargs.get('last_epoch', -1)
        div_factor = 100.0 
        final_div_factor = 1000.0
        
        # OneCycleLR çš„é»˜è®¤åŠ¨é‡è®¾ç½® (å¦‚æœä½ æ²¡æ”¹è¿‡çš„è¯)
        base_momentum = 0.85
        max_momentum = 0.95
        
        if last_step > -1:
            initial_lr_val = max_lr / div_factor
            min_lr_val = initial_lr_val / final_div_factor
            
            for group in self.optimizer.param_groups:
                # 1. è¡¥é½å­¦ä¹ ç‡å‚æ•°
                group.setdefault('initial_lr', initial_lr_val)
                group.setdefault('max_lr', max_lr)
                group.setdefault('min_lr', min_lr_val)
                
                # 2. è¡¥é½åŠ¨é‡å‚æ•° (è¿™æ¬¡æŠ¥é”™æ˜¯å› ä¸ºç¼ºè¿™ä¿©)
                group.setdefault('base_momentum', base_momentum)
                group.setdefault('max_momentum', max_momentum)

        self.scheduler = optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=max_lr,
            # epochs=epochs,
            total_steps=int(total_steps * 1.02),
            pct_start=0.1, # 10% çš„æ­¥æ•°ç”¨äºå‡é«˜å­¦ä¹ ç‡
            div_factor=100.0, # åˆå§‹ lr ä¸º max_lr / div_factor
            final_div_factor=1000.0, # æœ€ç»ˆ lr ä¸º max_lr / final_div_factor
            anneal_strategy='cos',
            last_epoch=last_step
        )
        
        # Loss é…ç½®
        self.huber_delta = 0.01
        self.w_e = 1.0
        self.w_f = 10.0
        self.w_s = 10.0
        
        # è·å– rank
        self.rank = dist.get_rank() if dist.is_initialized() else 0
        self.checkpoint_dir = checkpoint_dir
        self.train_log_path = os.path.join(self.checkpoint_dir, 'train_log.csv')
        self.val_log_path = os.path.join(self.checkpoint_dir, 'val_log.csv')
        self.EV_A3_TO_GPA = 160.21766 
        
        # æ—¥å¿—åˆå§‹åŒ–
        if self.rank == 0:
            os.makedirs(self.checkpoint_dir, exist_ok=True)
            self._init_loggers()

    def _init_loggers(self):
        headers = ['epoch', 'step', 'lr', 'total_loss', 'loss_e', 'loss_f', 'loss_s', 'mae_e', 'mae_f', 'mae_s_gpa']
        for path in [self.train_log_path, self.val_log_path]:
            with open(path, 'w', newline='') as f:
                csv.writer(f).writerow(headers)

    def log_to_csv(self, mode, data):
        # åªæœ‰ rank 0 å†™å…¥
        if self.rank != 0:
            return
        path = self.train_log_path if mode == 'train' else self.val_log_path
        with open(path, 'a', newline='') as f:
            csv.writer(f).writerow([
                data['epoch'], data['step'], f"{data['lr']:.2e}",
                f"{data['total_loss']:.6f}", f"{data['loss_e']:.6f}",
                f"{data['loss_f']:.6f}", f"{data['loss_s']:.6f}",
                f"{data['mae_e']*1000:.6f}", f"{data['mae_f']*1000:.6f}", f"{data['mae_s_gpa']:.6f}"
            ])

    def step(self, batch, train=True, batch_idx=0):
        # ğŸ”¥ ä½¿ç”¨ non_blocking åŠ é€Ÿä¼ è¾“
        batch = batch.to(self.device, non_blocking=True)
        
        # --- 1. å¼€å¯æ¢¯åº¦ ---
        batch.pos.requires_grad_(True)
        if hasattr(batch, 'cell') and batch.cell is not None:
            batch.cell.requires_grad_(True) 
        
        # --- 2. æ„é€ è™šæ‹Ÿåº”å˜ ---
        # ğŸ”¥ ä¼˜åŒ– 2: æ¶ˆé™¤ Syncï¼Œä¼˜å…ˆè¯»å– PyG çš„ batch.num_graphs å±æ€§
        # è¿™é¿å…äº† .max().item() å¯¼è‡´çš„ CPU-GPU å¼ºåˆ¶åŒæ­¥
        if hasattr(batch, 'num_graphs'):
            num_graphs = batch.num_graphs
        else:
            # å…œåº•æ–¹æ¡ˆ
            num_graphs = int(batch.batch.max()) + 1
            
        displacement = torch.zeros((num_graphs, 3, 3), dtype=batch.pos.dtype, device=self.device)
        displacement.requires_grad_(True)
        symmetric_strain = 0.5 * (displacement + displacement.transpose(-1, -2))
        
        # --- 3. åº”ç”¨å˜å½¢ ---
        strain_per_atom = symmetric_strain[batch.batch]
        pos_deformed = batch.pos + torch.einsum('ni,nij->nj', batch.pos, strain_per_atom)
        
        original_pos = batch.pos
        original_cell = getattr(batch, 'cell', None)
        
        batch.pos = pos_deformed
        
        if original_cell is not None and original_cell.dim() == 3:
            cell_deformed = original_cell + torch.bmm(original_cell, symmetric_strain)
            batch.cell = cell_deformed
        else:
            # è¿™é‡Œçš„æ‰“å°åœ¨å¤šå¡ç¯å¢ƒä¸‹å¯èƒ½ä¼šæœ‰ç‚¹ä¹±ï¼Œä½†ä¿ç•™åŸé€»è¾‘
            # print("âš ï¸ Warning: batch.cell is None or not 3D, skipping cell deformation.")
            pass
 
        # --- 4. å‰å‘ä¼ æ’­ ---
        pred_e = self.model(batch).view(-1)
        
        # æ¢å¤åŸå§‹åæ ‡
        batch.pos = original_pos
        if original_cell is not None: batch.cell = original_cell
        
        # --- 5. è‡ªåŠ¨æ±‚å¯¼è®¡ç®—åŠ›ä¸åº”åŠ› ---
        grad_out = torch.ones_like(pred_e)
        grads = torch.autograd.grad(
            outputs=pred_e, 
            inputs=[original_pos, displacement], 
            grad_outputs=grad_out,
            create_graph=train, 
            retain_graph=train,
            allow_unused=True
        )
        
        pred_f = -grads[0] if grads[0] is not None else torch.zeros_like(batch.pos)
        dE_dStrain = grads[1]

        # --- 6. ä¿®æ­£ä½“ç§¯è®¡ç®—ä¸å®‰å…¨é™¤æ³• ---
        if original_cell is not None:
            vol = torch.abs(torch.linalg.det(original_cell)).view(-1, 1, 1)
        else:
            vol = torch.ones_like(dE_dStrain)

        # ğŸ›¡ï¸ å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢æ¢¯åº¦æ–­è¿å¯¼è‡´ dE_dStrain ä¸º None æ—¶æŠ¥é”™
        if dE_dStrain is not None:
            pred_stress = dE_dStrain / vol
        else:
            # ä¿æŒç»´åº¦ä¸€è‡´ (Batch, 3, 3)
            pred_stress = torch.zeros((num_graphs, 3, 3), device=self.device)
        
        # ==================================================================
        # 6. Loss è®¡ç®— 
        # ==================================================================
        target_e = batch.y.view(-1)
        
        # ç¼“å­˜ scatter buffer é¿å…é‡å¤åˆ›å»º (å¾®å°ä¼˜åŒ–)
        if not hasattr(self, '_ones_buffer') or self._ones_buffer.shape[0] != batch.batch.shape[0]:
             self._ones_buffer = torch.ones_like(batch.batch, dtype=torch.float64)
        
        num_atoms = scatter_add(self._ones_buffer, batch.batch, dim=0, dim_size=num_graphs).view(-1).clamp(min=1)
        
        loss_e = F.huber_loss(pred_e / num_atoms, target_e / num_atoms, delta=self.huber_delta)
        
        # ä½¿ç”¨ JIT åŠ é€Ÿåçš„ Loss
        loss_f = conditional_huber_loss(pred_f, batch.force, base_delta=self.huber_delta)
        
        loss_s = torch.tensor(0.0, device=self.device, requires_grad=train)
        stress_mask_sum = 0
        if hasattr(batch, 'stress') and batch.stress is not None:
            stress_norm = torch.norm(batch.stress.view(num_graphs, -1), dim=1)
            stress_mask = (stress_norm > 1e-6)
            stress_mask_sum = stress_mask.sum().item() # è¿™é‡Œå¿…é¡»åŒæ­¥è·å–æ•°å€¼ç”¨äºåˆ¤æ–­
            if stress_mask_sum > 0:
                s_pred = pred_stress.view(num_graphs, -1)[stress_mask]
                s_target = batch.stress.view(num_graphs, -1)[stress_mask]
                loss_s = F.huber_loss(s_pred, s_target, delta=self.huber_delta)

        total_loss = self.w_e * loss_e + self.w_f * loss_f + self.w_s * loss_s
        
        # --- 7. Metrics è®¡ç®— ---
        with torch.no_grad():
            mae_e = F.l1_loss(pred_e / num_atoms, target_e / num_atoms).item()
            mae_f = F.l1_loss(pred_f, batch.force).item()
            mae_s_gpa = 0.0
            if stress_mask_sum > 0:
                mae_s_val = F.l1_loss(
                    pred_stress.view(num_graphs, -1)[stress_mask], 
                    batch.stress.view(num_graphs, -1)[stress_mask]
                )
                mae_s_gpa = mae_s_val.item() * self.EV_A3_TO_GPA

        # --- 8. åå‘ä¼ æ’­ä¸ä¼˜åŒ– ---
        if train:
            self.optimizer.zero_grad(set_to_none=True)
            total_loss.backward()
            nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            # ğŸ”¥ ä¼˜åŒ– 3: EMA é™é¢‘æ›´æ–° (æ¯ 5 æ­¥ä¸€æ¬¡)
            if batch_idx % 5 == 0:
                self.ema.update()
            
        return {
            'total_loss': total_loss.item(),
            'loss_e': loss_e.item(), 'loss_f': loss_f.item(), 'loss_s': loss_s.item(),
            'mae_e': mae_e, 'mae_f': mae_f, 'mae_s_gpa': mae_s_gpa
        }

    def train_epoch(self, loader, epoch_idx):
        self.model.train()
        pbar = tqdm(loader, desc=f"Train Ep {epoch_idx}", leave=False, disable=(self.rank != 0))
        metrics_sum = {'mae_e': 0, 'mae_f': 0, 'mae_s_gpa': 0, 'total_loss': 0}
        count = 0
        
        # ğŸ”¥ ä½¿ç”¨ enumerate è·å– batch_idx
        for i, batch in enumerate(pbar):
            if i == 0:
                if self.rank == 0:
                    print("First batch graph info:")
                    print("Number of graphs in batch:", batch.num_graphs)
                    print("Nodes (atoms) in batch:", batch.pos.size(0))
                    print("Edge index:", batch.edge_index)
                    print("Batch indices:", batch.batch)
                    # çœ‹stressæ˜¯ä¸æ˜¯ä¸æ˜¯Noneå’Œç©º
                    if hasattr(batch, 'stress') and batch.stress is not None:
                        print("Stress tensor shape:", batch.stress.shape)
                    else:
                        print("No stress tensor in this batch.")

            # ä¼ å…¥ batch_idx æ§åˆ¶ EMA æ›´æ–°é¢‘ç‡
            metrics = self.step(batch, train=True, batch_idx=i)
            
            if i == 0 and self.rank == 0:
                # ä½ çš„ debug æ‰“å°é€»è¾‘ä¿æŒä¸å˜
                pass 
            
            # è®°å½• CSV (ä½ è¦æ±‚æ¯ä¸€æ­¥éƒ½ä¿ç•™ I/O)
            log_data = metrics.copy()
            log_data.update({'epoch': epoch_idx, 'step': i, 'lr': self.optimizer.param_groups[0]['lr']})
            self.log_to_csv('train', log_data)
            
            self.scheduler.step()
            
            # ç»Ÿè®¡
            for k in metrics_sum: metrics_sum[k] += metrics[k]
            count += 1
            pbar.set_postfix({'L': f"{metrics['total_loss']:.4f}", 
                              'MAE_e': f"{metrics['mae_e']*1000:.1f}",
                              'MAE_F': f"{metrics['mae_f']*1000:.1f}"})
            
        return {k: v/count for k,v in metrics_sum.items()}

    def validate(self, loader, epoch_idx):
        self.model.eval()
        pbar = tqdm(loader, desc=f"Val Ep {epoch_idx}", leave=False, disable=(self.rank != 0))
        metrics_sum = {'mae_e': 0, 'mae_f': 0, 'mae_s_gpa': 0, 'total_loss': 0}
        count = 0
        
        with self.ema.average_parameters():
            with torch.set_grad_enabled(True):
                for i, batch in enumerate(pbar):
                    metrics = self.step(batch, train=False)
                    
                    log_data = metrics.copy()
                    log_data.update({'epoch': epoch_idx, 'step': i, 'lr': self.optimizer.param_groups[0]['lr']})
                    self.log_to_csv('val', log_data)
                    
                    for k in metrics_sum: metrics_sum[k] += metrics[k]
                    count += 1
                    pbar.set_postfix({'L': f"{metrics['total_loss']:.4f}", 
                                      'MAE_e': f"{metrics['mae_e']*1000:.1f}",
                                      'MAE_F': f"{metrics['mae_f']*1000:.1f}"})
        
        if count == 0: count = 1
        return {k: v/count for k,v in metrics_sum.items()}

    def save(self, filename='best_model.pt', rank = 0):
            path = os.path.join(self.checkpoint_dir, filename)

            # 1. è§£å¼€ DDP åŒ…è£… (å¦‚æœä½ ç”¨äº†å¤šå¡)
            # å¦‚æœæ˜¯ DDPï¼ŒçœŸå®çš„æ¨¡å‹è—åœ¨ .module é‡Œï¼›å¦‚æœæ˜¯å•å¡ï¼Œå°±æ˜¯ self.model
            raw_model = self.model.module if hasattr(self.model, 'module') else self.model

            # 2. å¼€å¯ EMA ä¸Šä¸‹æ–‡
            # åœ¨è¿™ä¸ª block é‡Œï¼Œæ¨¡å‹çš„å‚æ•°è¢«ä¸´æ—¶æ›¿æ¢æˆäº† EMA å¹³æ»‘åçš„å‚æ•°
            with self.ema.average_parameters():
                
                # 3. å‡†å¤‡è¦ä¿å­˜çš„å­—å…¸ (åŒ…å«é…ç½®ï¼)
                checkpoint = {
                    'model_state_dict': raw_model.state_dict(), # ğŸ‘ˆ å­˜çš„æ˜¯ EMA æƒé‡
                    'model_config': getattr(raw_model, 'cfg', None) # ğŸ‘ˆ å­˜é…ç½® (è‡ªåŠ¨åŠ è½½çš„å…³é”®)
                }
                
                # 4. ä¿å­˜æ–‡ä»¶
                torch.save(checkpoint, path)

                if rank == 0:
                    print(f"âœ… Model saved to {path} with config!")
