import torch
import random
from torch.utils.data import Sampler

class BinPackingSampler(Sampler):
    def __init__(self, metadata, max_cost=3000, edge_weight='auto', shuffle=True, 
                 world_size=1, rank=0, seed=42): # ğŸ”¥ æ–°å¢ seed å‚æ•°
        """
        :param seed: åŸºç¡€éšæœºç§å­ï¼Œä¿è¯ DDP å„å¡åˆå§‹çŠ¶æ€ä¸€è‡´
        """
        self.metadata = metadata
        self.max_cost = max_cost
        self.shuffle = shuffle
        self.world_size = world_size
        self.rank = rank
        self.seed = seed      # ğŸ”¥ ä¿å­˜ç§å­
        self.epoch = 0        # ğŸ”¥ æ–°å¢ epoch è®¡æ•°å™¨
        
        # ---------------------------------------------------
        # 1. è®¡ç®—æƒé‡ (é€»è¾‘ä¿æŒä¸å˜)
        # ---------------------------------------------------
        if edge_weight == 'auto':
            total_atoms = 0
            total_edges = 0
            for item in metadata:
                total_atoms += item['num_atoms']
                total_edges += item['num_edges']
            
            if total_edges > 0:
                self.edge_weight = total_atoms / total_edges
            else:
                self.edge_weight = 0.0
            
            # ä»…åœ¨ä¸»è¿›ç¨‹æ‰“å°
            if self.rank == 0:
                print(f"âš–ï¸ [Auto-Balance] Total Atoms: {total_atoms}, Total Edges: {total_edges}")
                print(f"âš–ï¸ [Auto-Balance] Calculated Edge Weight: {self.edge_weight:.6f}")
                print(f"   (è¿™æ„å‘³ç€æ¯ {1/self.edge_weight:.1f} æ¡è¾¹ â‰ˆ 1 ä¸ªåŸå­çš„æ˜¾å­˜æ¶ˆè€—)")
        else:
            self.edge_weight = float(edge_weight)

        # ---------------------------------------------------
        # 2. é¢„è®¡ç®—æ‰€æœ‰ Cost
        # ---------------------------------------------------
        self.indices_with_cost = []
        for i, item in enumerate(metadata):
            # Cost = Atoms + æƒé‡ * Edges
            c = item['num_atoms'] + self.edge_weight * item['num_edges']
            self.indices_with_cost.append((i, c))

    def set_epoch(self, epoch):
        """
        ğŸ”¥ å…³é”®æ–¹æ³•ï¼šåœ¨æ¯ä¸ª Epoch å¼€å§‹å‰è°ƒç”¨ï¼Œ
        ç¡®ä¿æ¯ä¸€è½®çš„éšæœºæ‰°åŠ¨ä¸åŒï¼Œä½†åœ¨æ‰€æœ‰ GPU ä¸Šæ˜¯ä¸€è‡´çš„ã€‚
        """
        self.epoch = epoch

    def _generate_batches(self, epoch_idx):
        """
        ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šå°†ç”Ÿæˆé€»è¾‘æŠ½ç¦»ï¼Œä½¿å…¶å¯ä»¥è¢«æ¨¡æ‹Ÿè°ƒç”¨
        è¿”å›ï¼šå½“å‰ Rank åœ¨æŒ‡å®š epoch åº”è¯¥æ‹¿åˆ°çš„ batch åˆ—è¡¨
        """
        rng = random.Random(self.seed + epoch_idx) # ç¡®å®šæ€§éšæœº

        # 1. å¤åˆ¶å¹¶æ’åº
        indices = self.indices_with_cost.copy()
        if self.shuffle:
            indices.sort(key=lambda x: x[1] * rng.uniform(0.99, 1.01), reverse=True)
        else:
            indices.sort(key=lambda x: x[1], reverse=True)

        # 2. è£…ç®±
        batches = []
        current_batch = []
        current_batch_cost = 0
        
        for idx, cost in indices:
            if current_batch_cost + cost > self.max_cost and current_batch:
                batches.append(current_batch)
                current_batch = []
                current_batch_cost = 0
            current_batch.append(idx)
            current_batch_cost += cost
        if current_batch:
            batches.append(current_batch)

        # 3. Batch é—´ Shuffle
        if self.shuffle:
            rng.shuffle(batches)

        # 4. DDP åˆ‡ç‰‡ (Drop Last é€»è¾‘)
        total_batches = len(batches)
        num_samples_per_rank = total_batches // self.world_size
        batches = batches[:num_samples_per_rank * self.world_size]
        my_batches = batches[self.rank::self.world_size]
        
        return my_batches

    def __iter__(self):
        # ç›´æ¥è°ƒç”¨æŠ½ç¦»çš„é€»è¾‘
        batches = self._generate_batches(self.epoch)
        for batch in batches:
            yield batch

    def __len__(self):
        # è¿™ä¸ª len ä¾ç„¶åªèƒ½è¿”å›ä¼°è®¡å€¼æˆ–å½“å‰ epoch çš„å€¼
        # ä½†æ—¢ç„¶æˆ‘ä»¬è¦ç²¾ç¡®è®¡ç®—æ€»æ­¥æ•°ï¼Œè¿™ä¸ª len å¯¹ Scheduler å·²ç»ä¸é‡è¦äº†ï¼Œåªå¯¹ tqdm æœ‰ç”¨
        return len(self._generate_batches(self.epoch))

    def precompute_total_steps(self, total_epochs):
        """
        ğŸ”¥ æ–°å¢æ–¹æ³•ï¼šç²¾ç¡®è®¡ç®—æœªæ¥æ‰€æœ‰ Epoch çš„æ­¥æ•°æ€»å’Œ
        """
        if self.rank == 0:
            print(f"ğŸ”„ Pre-computing exact steps for {total_epochs} epochs...")
        
        total_steps = 0
        for ep in range(total_epochs):
            # æ¨¡æ‹Ÿç”Ÿæˆæ¯ä¸€è½®çš„ batch (è®¡ç®—æå¿«ï¼Œå› ä¸ºåªæ˜¯æ“ä½œæ•´æ•°åˆ—è¡¨)
            batches = self._generate_batches(ep)
            total_steps += len(batches)
            
        if self.rank == 0:
            print(f"âœ… Exact total steps: {total_steps} (Avg: {total_steps/total_epochs:.1f})")
        
        return total_steps